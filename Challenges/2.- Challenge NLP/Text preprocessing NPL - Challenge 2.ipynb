{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"color:Blue\">Programacion 2</span> </center>\n",
    "<center> <span style=\"color:Gray\">  Challenge 2: Analizing Comments on Glassdoor </span>  </center>\n",
    "<center> <span style=\"color:Gray\"> Ilse Arredondo Reyes. No. Alumno 323019078</span>  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Stages  \n",
    "\n",
    "**Stage 1: Web Scrapping**  \n",
    "\n",
    "Will create a pipeline to extract Information of website Glassdoor\n",
    "â—‹ Web Scraping\n",
    "This endpoint will receive as input Web scrapping:\n",
    "- Open web page\n",
    "- Read the content\n",
    "- Extract content\n",
    "- Save all in a data frame\n",
    "\n",
    "----\n",
    "\n",
    "**Stage 2: Model Creation**\n",
    "\n",
    "ðŸ”¹ **Text Preprocessing** \n",
    "\n",
    "This endpoint will receive as input Web scrapping result in a data frame\n",
    "- Create English-Spanish data frame\n",
    "- Analyze Separately Spanish and English data frame (Corpus)\n",
    "- Data cleaning\n",
    "- Stop words.\n",
    "- Lemmatization\n",
    "- N-grmas Distributions\n",
    "\n",
    "ðŸ”¹ **Classification proposed**\n",
    "\n",
    "This endpoint will receive as input parameters of text preprocessing:\n",
    "\n",
    "- Construction model\n",
    "- End Date (date time)\n",
    "- Taring model\n",
    "- Calculation of grammatical probabilities\n",
    "\n",
    "ðŸ”¹ **Extraction of main features**\n",
    "- Classification\n",
    "- Sentimental Analysis (pysentimiento vs vader)\n",
    "\n",
    "---\n",
    "\n",
    "**Stage 3: Create a pipeline to MLOps**\n",
    "\n",
    "ðŸ”¹ **The MLOps part will be done with mlflow performing the following tasks**\n",
    "\n",
    "- **Log metrics**\n",
    "- **Model signatures**\n",
    "- **Save the plot and log it as an artifact**\n",
    "- **Tracking url (localhost)**\n",
    "- **Run MLOps**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Company, Location]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_glassdoor(url):\n",
    "    # Setup Selenium options (headless)\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    try:\n",
    "        # Open the Glassdoor page\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # wait for page to load completely\n",
    "        \n",
    "        # Read page content\n",
    "        html = driver.page_source\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Example: Extract job titles (customize selectors based on Glassdoor structure)\n",
    "        job_titles = [tag.get_text(strip=True) for tag in soup.select('a.jobLink span')]\n",
    "        companies = [tag.get_text(strip=True) for tag in soup.select('div.jobHeader span')]\n",
    "        locations = [tag.get_text(strip=True) for tag in soup.select('span.subtle.loc')]\n",
    "        \n",
    "        # Combine data into a DataFrame\n",
    "        data = pd.DataFrame({\n",
    "            'Job Title': job_titles,\n",
    "            'Company': companies,\n",
    "            'Location': locations\n",
    "        })\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage\n",
    "url = 'https://www.glassdoor.com/Job/software-engineer-jobs-SRCH_KO0,17.htm'\n",
    "df = scrape_glassdoor(url)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Company, Location, cleaned_text]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ilse-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ilse-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ilse-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Assume `df` is your DataFrame from Stage 1\n",
    "def preprocess_text(df, text_column='Job Title'):\n",
    "    df = df.copy()\n",
    "    df.dropna(subset=[text_column], inplace=True)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words_en = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zÃ¡Ã©Ã­Ã³ÃºÃ±Ã¼ ]', '', text)  # keep letters and Spanish accents\n",
    "        tokens = text.split()\n",
    "        tokens = [t for t in tokens if t not in stop_words_en]\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    df['cleaned_text'] = df[text_column].apply(clean_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example\n",
    "df_cleaned = preprocess_text(df)\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English-Spanish Corpus Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "df_cleaned['lang'] = df_cleaned['cleaned_text'].apply(detect_language)\n",
    "df_en = df_cleaned[df_cleaned['lang'] == 'en']\n",
    "df_es = df_cleaned[df_cleaned['lang'] == 'es']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words_freq[:n]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Example for English unigrams and bigrams\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_top_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_en\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_top_ngrams(df_en[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m], n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mget_top_ngrams\u001b[1;34m(corpus, n, ngram_range)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_top_ngrams\u001b[39m(corpus, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)):\n\u001b[1;32m----> 2\u001b[0m     vec \u001b[38;5;241m=\u001b[39m \u001b[43mCountVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngram_range\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     bag_of_words \u001b[38;5;241m=\u001b[39m vec\u001b[38;5;241m.\u001b[39mtransform(corpus)\n\u001b[0;32m      4\u001b[0m     sum_words \u001b[38;5;241m=\u001b[39m bag_of_words\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\PROGRAMACION2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1327\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \n\u001b[0;32m   1314\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1327\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\PROGRAMACION2\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\PROGRAMACION2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\PROGRAMACION2\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1284\u001b[0m         )\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "def get_top_ngrams(corpus, n=None, ngram_range=(1, 2)):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Example for English unigrams and bigrams\n",
    "print(get_top_ngrams(df_en['cleaned_text'], n=10, ngram_range=(1,1)))\n",
    "print(get_top_ngrams(df_en['cleaned_text'], n=10, ngram_range=(2,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example: create labels based on job titles\n",
    "df_en['label'] = df_en['Job Title'].apply(lambda x: 'engineer' if 'engineer' in x.lower() else 'other')\n",
    "\n",
    "# Text Classification Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_en['cleaned_text'], df_en['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English: Vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_en['sentiment_vader'] = df_en['cleaned_text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "# Spanish: pysentimiento\n",
    "from pysentimiento import create_analyzer\n",
    "\n",
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "df_es['sentiment'] = df_es['cleaned_text'].apply(lambda x: analyzer.predict(x).output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROGRAMACION2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
