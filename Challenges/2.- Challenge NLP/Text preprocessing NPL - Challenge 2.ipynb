{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"color:Blue\">Programacion 2</span> </center>\n",
    "<center> <span style=\"color:Gray\">  Challenge 2: Analizing Comments on Glassdoor </span>  </center>\n",
    "<center> <span style=\"color:Gray\"> Ilse Arredondo Reyes. No. Alumno 323019078</span>  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Stages  \n",
    "\n",
    "**Stage 1: Web Scrapping**  \n",
    "\n",
    "Will create a pipeline to extract Information of website Glassdoor\n",
    "â—‹ Web Scraping\n",
    "This endpoint will receive as input Web scrapping:\n",
    "- Open web page\n",
    "- Read the content\n",
    "- Extract content\n",
    "- Save all in a data frame\n",
    "\n",
    "----\n",
    "\n",
    "**Stage 2: Model Creation**\n",
    "\n",
    "ðŸ”¹ **Text Preprocessing** \n",
    "\n",
    "This endpoint will receive as input Web scrapping result in a data frame\n",
    "- Create English-Spanish data frame\n",
    "- Analyze Separately Spanish and English data frame (Corpus)\n",
    "- Data cleaning\n",
    "- Stop words.\n",
    "- Lemmatization\n",
    "- N-grmas Distributions\n",
    "\n",
    "ðŸ”¹ **Classification proposed**\n",
    "\n",
    "This endpoint will receive as input parameters of text preprocessing:\n",
    "\n",
    "- Construction model\n",
    "- End Date (date time)\n",
    "- Taring model\n",
    "- Calculation of grammatical probabilities\n",
    "\n",
    "ðŸ”¹ **Extraction of main features**\n",
    "- Classification\n",
    "- Sentimental Analysis (pysentimiento vs vader)\n",
    "\n",
    "---\n",
    "\n",
    "**Stage 3: Create a pipeline to MLOps**\n",
    "\n",
    "ðŸ”¹ **The MLOps part will be done with mlflow performing the following tasks**\n",
    "\n",
    "- **Log metrics**\n",
    "- **Model signatures**\n",
    "- **Save the plot and log it as an artifact**\n",
    "- **Tracking url (localhost)**\n",
    "- **Run MLOps**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Company, Location]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_glassdoor(url):\n",
    "    # Setup Selenium options (headless)\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    try:\n",
    "        # Open the Glassdoor page\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # wait for page to load completely\n",
    "        \n",
    "        # Read page content\n",
    "        html = driver.page_source\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Example: Extract job titles (customize selectors based on Glassdoor structure)\n",
    "        job_titles = [tag.get_text(strip=True) for tag in soup.select('a.jobLink span')]\n",
    "        companies = [tag.get_text(strip=True) for tag in soup.select('div.jobHeader span')]\n",
    "        locations = [tag.get_text(strip=True) for tag in soup.select('span.subtle.loc')]\n",
    "        \n",
    "        # Combine data into a DataFrame\n",
    "        data = pd.DataFrame({\n",
    "            'Job Title': job_titles,\n",
    "            'Company': companies,\n",
    "            'Location': locations\n",
    "        })\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage\n",
    "url = 'https://www.glassdoor.com/Job/software-engineer-jobs-SRCH_KO0,17.htm'\n",
    "df = scrape_glassdoor(url)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangdetect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ngrams\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "# --- Utility functions ---\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def preprocess_text(text, lang):\n",
    "    if lang == \"en\":\n",
    "        doc = nlp_en(text)\n",
    "        return [token.lemma_ for token in doc if token.lemma_ not in stopwords_en and not token.is_punct and not token.is_space]\n",
    "    elif lang == \"es\":\n",
    "        doc = nlp_es(text)\n",
    "        return [token.lemma_ for token in doc if token.lemma_ not in stopwords_es and not token.is_punct and not token.is_space]\n",
    "    return []\n",
    "\n",
    "def extract_ngrams(tokens, n=2):\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "def get_ngram_distribution(token_lists, n=2, top_n=10):\n",
    "    all_ngrams = []\n",
    "    for tokens in token_lists:\n",
    "        all_ngrams.extend(extract_ngrams(tokens, n))\n",
    "    return Counter(all_ngrams).most_common(top_n)\n",
    "\n",
    "# --- Main processing function ---\n",
    "\n",
    "def text_preprocessing_pipeline(df, text_column=\"Job Title\"):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Clean text and detect language\n",
    "    df[\"Clean Text\"] = df[text_column].fillna(\"\").apply(clean_text)\n",
    "    df[\"Lang\"] = df[\"Clean Text\"].apply(detect_language)\n",
    "\n",
    "    # Process text: tokenize, lemmatize, remove stopwords\n",
    "    df[\"Tokens\"] = df.apply(lambda row: preprocess_text(row[\"Clean Text\"], row[\"Lang\"]), axis=1)\n",
    "    \n",
    "    # Extract bigrams\n",
    "    df[\"Bigrams\"] = df[\"Tokens\"].apply(lambda tokens: extract_ngrams(tokens, n=2))\n",
    "\n",
    "    # Split by language\n",
    "    df_en = df[df[\"Lang\"] == \"en\"].reset_index(drop=True)\n",
    "    df_es = df[df[\"Lang\"] == \"es\"].reset_index(drop=True)\n",
    "\n",
    "    # N-gram distributions\n",
    "    bigrams_en = get_ngram_distribution(df_en[\"Tokens\"], n=2)\n",
    "    bigrams_es = get_ngram_distribution(df_es[\"Tokens\"], n=2)\n",
    "\n",
    "    results = {\n",
    "        \"full_df\": df,\n",
    "        \"english_df\": df_en,\n",
    "        \"spanish_df\": df_es,\n",
    "        \"english_bigrams_distribution\": bigrams_en,\n",
    "        \"spanish_bigrams_distribution\": bigrams_es,\n",
    "    }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.8.3-cp39-cp39-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: langdetect in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: nltk in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (3.9.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.12-cp39-cp39-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.11-cp39-cp39-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp39-cp39-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Using cached thinc-8.3.6-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.5.1-cp39-cp39-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: six in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached blis-1.3.0.tar.gz (2.5 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached marisa_trie-1.2.1-cp39-cp39-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\ilse-\\anaconda3\\envs\\programacion2\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached spacy-3.8.3-cp39-cp39-win_amd64.whl (12.3 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.11-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.12-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.1-cp39-cp39-win_amd64.whl (633 kB)\n",
      "Using cached thinc-8.3.6-cp39-cp39-win_amd64.whl (1.8 MB)\n",
      "Using cached typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached marisa_trie-1.2.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: blis\n",
      "  Building wheel for blis (pyproject.toml): started\n",
      "  Building wheel for blis (pyproject.toml): finished with status 'error'\n",
      "Failed to build blis\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— Building wheel for blis (pyproject.toml) did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [32 lines of output]\n",
      "      BLIS_COMPILER? None\n",
      "      C:\\Users\\ilse-\\AppData\\Local\\Temp\\pip-build-env-8fu8h48n\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: BSD License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\about.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\benchmark.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\__init__.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      creating build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "      copying blis\\tests\\common.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "      copying blis\\tests\\test_dotv.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "      copying blis\\tests\\test_gemm.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "      copying blis\\tests\\__init__.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "      copying blis\\cy.pyx -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\py.pyx -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\cy.pxd -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      copying blis\\__init__.pxd -> build\\lib.win-amd64-cpython-39\\blis\n",
      "      running build_ext\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for blis\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (blis)\n"
     ]
    }
   ],
   "source": [
    "pip install spacy langdetect nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROGRAMACION2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
