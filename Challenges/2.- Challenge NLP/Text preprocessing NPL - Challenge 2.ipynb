{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"color:Blue\">Programacion 2</span> </center>\n",
    "<center> <span style=\"color:Gray\">  Challenge 2: Analizing Comments on Glassdoor </span>  </center>\n",
    "<center> <span style=\"color:Gray\"> Ilse Arredondo Reyes. No. Alumno 323019078</span>  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Stages  \n",
    "\n",
    "**Stage 1: Web Scrapping**  \n",
    "\n",
    "Will create a pipeline to extract Information of website Glassdoor\n",
    "â—‹ Web Scraping\n",
    "This endpoint will receive as input Web scrapping:\n",
    "- Open web page\n",
    "- Read the content\n",
    "- Extract content\n",
    "- Save all in a data frame\n",
    "\n",
    "----\n",
    "\n",
    "**Stage 2: Model Creation**\n",
    "\n",
    "ðŸ”¹ **Text Preprocessing** \n",
    "\n",
    "This endpoint will receive as input Web scrapping result in a data frame\n",
    "- Create English-Spanish data frame\n",
    "- Analyze Separately Spanish and English data frame (Corpus)\n",
    "- Data cleaning\n",
    "- Stop words.\n",
    "- Lemmatization\n",
    "- N-grmas Distributions\n",
    "\n",
    "ðŸ”¹ **Classification proposed**\n",
    "\n",
    "This endpoint will receive as input parameters of text preprocessing:\n",
    "\n",
    "- Construction model\n",
    "- End Date (date time)\n",
    "- Taring model\n",
    "- Calculation of grammatical probabilities\n",
    "\n",
    "ðŸ”¹ **Extraction of main features**\n",
    "- Classification\n",
    "- Sentimental Analysis (pysentimiento vs vader)\n",
    "\n",
    "---\n",
    "\n",
    "**Stage 3: Create a pipeline to MLOps**\n",
    "\n",
    "ðŸ”¹ **The MLOps part will be done with mlflow performing the following tasks**\n",
    "\n",
    "- **Log metrics**\n",
    "- **Model signatures**\n",
    "- **Save the plot and log it as an artifact**\n",
    "- **Tracking url (localhost)**\n",
    "- **Run MLOps**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Company, Location]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_glassdoor(url):\n",
    "    # Setup Selenium options (headless)\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    try:\n",
    "        # Open the Glassdoor page\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # wait for page to load completely\n",
    "        \n",
    "        # Read page content\n",
    "        html = driver.page_source\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Example: Extract job titles (customize selectors based on Glassdoor structure)\n",
    "        job_titles = [tag.get_text(strip=True) for tag in soup.select('a.jobLink span')]\n",
    "        companies = [tag.get_text(strip=True) for tag in soup.select('div.jobHeader span')]\n",
    "        locations = [tag.get_text(strip=True) for tag in soup.select('span.subtle.loc')]\n",
    "        \n",
    "        # Combine data into a DataFrame\n",
    "        data = pd.DataFrame({\n",
    "            'Job Title': job_titles,\n",
    "            'Company': companies,\n",
    "            'Location': locations\n",
    "        })\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage\n",
    "url = 'https://www.glassdoor.com/Job/software-engineer-jobs-SRCH_KO0,17.htm'\n",
    "df = scrape_glassdoor(url)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilse-\\anaconda3\\envs\\PROGRAMACION2\\python.exe: No module named spacy\n",
      "c:\\Users\\ilse-\\anaconda3\\envs\\PROGRAMACION2\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangdetect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ngrams\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Load language models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "# Language detection\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# Lemmatization and stopword removal\n",
    "def preprocess_text(text, lang):\n",
    "    if lang == \"en\":\n",
    "        doc = nlp_en(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.lemma_ not in stopwords_en and not token.is_punct]\n",
    "    elif lang == \"es\":\n",
    "        doc = nlp_es(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.lemma_ not in stopwords_es and not token.is_punct]\n",
    "    else:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "# Extract n-grams\n",
    "def extract_ngrams(tokens, n=2):\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# Apply pipeline\n",
    "def preprocess_dataframe(df, text_column='Job Title'):\n",
    "    df['Clean Text'] = df[text_column].fillna('').apply(clean_text)\n",
    "    df['Lang'] = df['Clean Text'].apply(detect_language)\n",
    "    df['Tokens'] = df.apply(lambda row: preprocess_text(row['Clean Text'], row['Lang']), axis=1)\n",
    "    df['Bigrams'] = df['Tokens'].apply(lambda x: extract_ngrams(x, 2))\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROGRAMACION2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
